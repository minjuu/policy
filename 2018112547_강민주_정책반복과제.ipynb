{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nimport copy\nimport math\nfrom tqdm import tqdm\nimport random\nimport itertools\nimport pickle as cPickle",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 마크다운 문서\npl player : Human player\np2 player : Policy player\npossible actions =  [0, 1, 2, 3, 4, 5, 6, 7, 8]\n+----+----+----+\n+  0 +  1 +  2 +\n+----+----+----+\n+  3 +  4 +  5 +\n+----+----+----+\n+  6 +  7 +  8 +\n+----+----+----+\nSelect action(human) : 3\n+----+----+----+\n|    |    |    |\n+----+----+----+\n|  O |    |    |\n+----+----+----+\n|    |    |    |\n+----+----+----+\n+----+----+----+\n|  X |    |    |\n+----+----+----+\n|  O |    |    |\n+----+----+----+\n|    |    |    |\n+----+----+----+\npossible actions =  [1, 2, 4, 5, 6, 7, 8]\n+----+----+----+\n+  0 +  1 +  2 +\n+----+----+----+\n+  3 +  4 +  5 +\n+----+----+----+\n+  6 +  7 +  8 +\n+----+----+----+\nSelect action(human) : 5\n+----+----+----+\n|  X |    |    |\n+----+----+----+\n|  O |    |  O |\n+----+----+----+\n|    |    |    |\n+----+----+----+\n+----+----+----+\n|  X |    |    |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|    |    |    |\n+----+----+----+\npossible actions =  [1, 2, 6, 7, 8]\n+----+----+----+\n+  0 +  1 +  2 +\n+----+----+----+\n+  3 +  4 +  5 +\n+----+----+----+\n+  6 +  7 +  8 +\n+----+----+----+\nSelect action(human) : 8\n+----+----+----+\n|  X |    |    |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|    |    |  O |\n+----+----+----+\n+----+----+----+\n|  X |    |  X |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|    |    |  O |\n+----+----+----+\npossible actions =  [1, 6, 7]\n+----+----+----+\n+  0 +  1 +  2 +\n+----+----+----+\n+  3 +  4 +  5 +\n+----+----+----+\n+  6 +  7 +  8 +\n+----+----+----+\nSelect action(human) : 6\n+----+----+----+\n|  X |    |  X |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|  O |    |  O |\n+----+----+----+\n+----+----+----+\n|  X |  X |  X |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|  O |    |  O |\n+----+----+----+\nwinner is p2(Policy player)\nfinal result\n+----+----+----+\n|  X |  X |  X |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|  O |    |  O |\n+----+----+----+\nMore Game? (y/n)n\np1(Human player) = 0 p2(Policy player) = 1 draw = 0\n"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Tic Tac Toe 환경 정의"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Environment():\n    def __init__(self):\n        self.state = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n        self.go = 1\n\n    def set_reward(self, action):\n        self.state[action] = self.go\n        self.go = self.go % 2 + 1\n        done, winner = end_check(self.state)\n        reward = 0\n        if done and winner == 1: \n            reward = 1\n        if done and winner == 2: \n            reward = -1\n        if done and winner == 0: \n            reward = 0\n\n        return self.state, done, reward, winner\n\n    def setting(self):\n        self.state = [0, 0, 0, 0, 0, 0, 0, 0, 0]\n        self.go = 1\n\n    def print_board(self):\n        print(\"+----+----+----+\")\n        for i in range(3):\n            for j in range(3):\n                if self.state[3 * i + j] == 1:\n                    print(\"|  O\",end=\" \")\n                elif self.state[3 * i + j] == 2:\n                    print(\"|  X\",end=\" \")\n                else:\n                    print(\"|   \",end=\" \")\n            print(\"|\")\n            print(\"+----+----+----+\")\n            \n\ndef end_check(state):\n    for i in range(3):\n        if state[3 * i] == state[3 * i + 1] and state[3 * i + 1] == state[3 * i + 2] \\\n                and state[3 * i] != 0:\n            return True, state[3 * i]\n        if state[i] == state[3 + i] and state[3 + i] == state[6 + i] and state[i] != 0:\n            return True, state[i]\n    if state[0] == state[4] and state[4] == state[8] and state[0] != 0:\n        return True, state[0]\n    if state[2] == state[4] and state[4] == state[6] and state[2] != 0:\n        return True, state[2]\n    for i in range(9):\n        if state[i] == 0:\n            return False, 0\n    return True, 0\n\ndef bind(state):\n    getstr = str()\n    for x in state:\n        getstr += str(x)\n    return getstr\n\ndef available_action(state):\n    actionlist = []\n    for i in range(9):\n        if state[i] == 0:\n            actionlist.append(i)\n    return actionlist\n\ndef order(state):\n    c1 = c2 = 0\n    \n    for x in state:\n        if x == 1:\n            c1 += 1\n        elif x == 2:\n            c2 += 1\n    if c1 > c2:\n        go = 2\n    else:\n        go = 1\n\n    return go\n\n\ndef following(state, action):\n    state2 = copy.copy(state)\n    go = order(state)\n\n    state2[action] = go\n    done, winner = end_check(state2)\n\n    reward = 0\n    if done and winner == 1: reward = 1\n    if done and winner == 2: reward = -1\n    if done and winner == 0: reward = 0\n\n    return state2, reward\n\n",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Human player"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Human_player:    \n    def __init__(self):\n        self.name = \"Human player\"\n        \n    def policy(self, state):\n        available = available_action(state)\n\n        while True:\n            #가능한 행동을 조사한 후 표시\n            print(\"possible actions = \",available)\n            # 상태 번호 표시\n            print(\"+----+----+----+\")\n            print(\"+  0 +  1 +  2 +\")\n            print(\"+----+----+----+\")\n            print(\"+  3 +  4 +  5 +\")\n            print(\"+----+----+----+\")\n            print(\"+  6 +  7 +  8 +\")\n            print(\"+----+----+----+\")\n            getstr = int(input(\"Select action(human) : \"))\n            if getstr in available:\n                return getstr\n            else:\n                print(\"You selected wrong action\")\n        return",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 정책 반복 플레이어"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Policy_set:\n    def policy(self, state, go):\n        available = available_action(state)\n        action_list = []\n\n        for i in available:\n            state[i] = go\n            done, winner = end_check(state)\n            state[i] = 0\n            if done:\n                action_list.append(i)\n        if len(action_list) == 0:\n            action_list = available\n\n        return random.choice(action_list)\n\nclass Policy_player:\n    def __init__(self, rs_file=False):\n        self._value = dict()\n        self._policy = dict()\n        self.name = \"Policy player\"\n\n        if not rs_file:\n            self.init_value()\n        else:\n            self.rs_file()\n\n    def init_value(self):\n        statelist = itertools.product([0, 1, 2], repeat=9)\n\n        for state in statelist:\n            state = list(state)\n            binded = bind(state)\n            done, winner = end_check(state)\n            if not done:\n                self._value[binded] = random.uniform(-0.5, 0.5)\n                self._policy[binded] = random.choice(available_action(state))\n            # terminal state value\n            else:\n                self._value[binded] = 0\n                \n    def file_save(self):\n        file = open(\"v.dat\", 'wb')\n        cPickle.dump(self._value, file)\n        file = open(\"p.dat\", 'wb')\n        cPickle.dump(self._policy, file)\n\n    def rs_file(self):\n        file = open(\"v.dat\", 'rb')\n        self._value = cPickle.load(file)\n        file = open(\"p.dat\", 'rb')\n        self._policy = cPickle.load(file)\n\n    def value(self, state):\n        binded = bind(state)\n        return self._value[binded]\n\n    def renderv(self, state, x):\n        binded = bind(state)\n        self._value[binded] = x\n\n    def policy(self, state):\n        binded = bind(state)\n        return self._policy[binded]\n\n    def renderp(self, state, x):\n        binded = bind(state)\n        self._policy[binded] = x\n\nepisode = 100\ngamma = 0.9\n\nenv = Environment()\nagent = Policy_player(rs_file=False)\nstrucag = Policy_set()\n\niteration_arr = []\nv_arr = []\nwr_arr = []\n\ndef policyeval(agent):\n    theta = 1e-9\n\n    while True:\n        delta = 0.0\n\n        statelist = itertools.product([0, 1, 2], repeat=9)\n        for state in statelist:\n            state = list(state)\n            done, winner = end_check(state)\n            if not done:  # except for terminal state\n                v = agent.value(state)\n\n                action = agent.policy(state)\n                state2, reward = following(state, action)\n                agent.renderv(state, reward + gamma * agent.value(state2))\n\n                delta = max([delta, abs(v - agent.value(state))])\n\n        if delta < theta:\n            break\n\n\ndef polcyimp(agent):\n    stable = True\n\n    statelist = itertools.product([0, 1, 2], repeat=9)\n    for state in statelist:\n        state = list(state)\n        done, winner = end_check(state)\n        if not done:  # except for terminal state\n            available = available_action(state)\n            go = order(state)\n\n            preaction = agent.policy(state)\n\n            valuemax = -9999999\n            valuemin = 9999999\n            if go == 1:\n                for action in available:\n                    state2, reward = following(state, action)\n                    value = reward + gamma * agent.value(state2)\n                    if value > valuemax:\n                        valuemax = value\n                        naction = action\n            else:\n                for action in available:\n                    state2, reward = following(state, action)\n                    value = reward + gamma * agent.value(state2)\n                    if value < valuemin:\n                        valuemin = value\n                        naction = action\n\n            agent.renderp(state, naction)\n\n            if preaction != naction:\n                stable = False\n\n    if stable:\n        return True\n    else:\n        return False\n\n\ndef policytrain():\n    iteration = 0\n\n    while True:\n        iteration += 1\n        avg = 0.0\n\n        # policy iteration\n        policyeval(agent)\n        stop = polcyimp(agent)\n\n        # verification stage\n        win = lose = draw = 0\n        tcount = 0\n        for i in range(episode):\n            done = 0\n            env.setting()\n            state = copy.copy(env.state)\n\n            winner = 0\n            j = 0\n            while not done:\n                j += 1\n                go = copy.copy(env.go)\n                if (i + j) % 2 == 1:\n                    action = agent.policy(state)\n                    avg += agent.value(state) * ((-1) ** (go + 1))\n                    tcount += 1\n                else:\n                    action = strucag.policy(state, go)\n                state2, done, reward, winner = env.set_reward(action)\n                state = copy.copy(state2)\n\n            if winner == 0:\n                draw += 1\n            elif (i + j) % 2 == 1:\n                win += 1\n            else:\n                lose += 1\n        rwin = (win + draw) / episode\n        avg /= tcount\n\n        agent.file_save()\n        iteration_arr.append(iteration)\n        v_arr.append(avg)\n        wr_arr.append(rwin)\n\n        if stop:\n            break\n\n\nif __name__ == \"__main__\":\n    policytrain()",
      "execution_count": 4,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 게임 진행 함수"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "env = Environment()\n\np1 = Human_player()\np2 = Policy_player(rs_file=True)\n\n# 각 플레이어의 승리 횟수를 저장\np1_score = 0\np2_score = 0\ndraw_score = 0\n\nprint(\"pl player : {}\".format(p1.name))\nprint(\"p2 player : {}\".format(p2.name))\n\nwhile True:\n    done = 0\n    winner = 0\n    env.setting()\n    state = copy.copy(env.state)\n\n    i = 0\n    while not done:\n        i += 1\n        if i % 2 == 1:\n            action = p1.policy(state)\n        else:\n            action = p2.policy(state)\n        state2, done, reward, winner = env.set_reward(action)\n        state = copy.copy(state2)\n        env.print_board()\n\n    if reward == 0:\n        print(\"draw\")\n        draw_score += 1\n    elif reward == 1:\n        print(\"winner is p1({})\".format(p1.name))\n        p1_score += 1\n    else :\n        print(\"winner is p2({})\".format(p2.name))\n        p2_score += 1\n        \n    # 최종 결과 출력        \n    print(\"final result\")\n    env.print_board()\n\n   # 한게임 더?최종 결과 출력 \n    answer = input(\"More Game? (y/n)\")\n    if answer == 'n':\n        break        \n\nprint(\"p1({}) = {} p2({}) = {} draw = {}\".format(p1.name, p1_score,p2.name, p2_score,draw_score))\n                \n",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": "pl player : Human player\np2 player : Policy player\npossible actions =  [0, 1, 2, 3, 4, 5, 6, 7, 8]\n+----+----+----+\n+  0 +  1 +  2 +\n+----+----+----+\n+  3 +  4 +  5 +\n+----+----+----+\n+  6 +  7 +  8 +\n+----+----+----+\nSelect action(human) : 3\n+----+----+----+\n|    |    |    |\n+----+----+----+\n|  O |    |    |\n+----+----+----+\n|    |    |    |\n+----+----+----+\n+----+----+----+\n|  X |    |    |\n+----+----+----+\n|  O |    |    |\n+----+----+----+\n|    |    |    |\n+----+----+----+\npossible actions =  [1, 2, 4, 5, 6, 7, 8]\n+----+----+----+\n+  0 +  1 +  2 +\n+----+----+----+\n+  3 +  4 +  5 +\n+----+----+----+\n+  6 +  7 +  8 +\n+----+----+----+\nSelect action(human) : 5\n+----+----+----+\n|  X |    |    |\n+----+----+----+\n|  O |    |  O |\n+----+----+----+\n|    |    |    |\n+----+----+----+\n+----+----+----+\n|  X |    |    |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|    |    |    |\n+----+----+----+\npossible actions =  [1, 2, 6, 7, 8]\n+----+----+----+\n+  0 +  1 +  2 +\n+----+----+----+\n+  3 +  4 +  5 +\n+----+----+----+\n+  6 +  7 +  8 +\n+----+----+----+\nSelect action(human) : 8\n+----+----+----+\n|  X |    |    |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|    |    |  O |\n+----+----+----+\n+----+----+----+\n|  X |    |  X |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|    |    |  O |\n+----+----+----+\npossible actions =  [1, 6, 7]\n+----+----+----+\n+  0 +  1 +  2 +\n+----+----+----+\n+  3 +  4 +  5 +\n+----+----+----+\n+  6 +  7 +  8 +\n+----+----+----+\nSelect action(human) : 6\n+----+----+----+\n|  X |    |  X |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|  O |    |  O |\n+----+----+----+\n+----+----+----+\n|  X |  X |  X |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|  O |    |  O |\n+----+----+----+\nwinner is p2(Policy player)\nfinal result\n+----+----+----+\n|  X |  X |  X |\n+----+----+----+\n|  O |  X |  O |\n+----+----+----+\n|  O |    |  O |\n+----+----+----+\nMore Game? (y/n)n\np1(Human player) = 0 p2(Policy player) = 1 draw = 0\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "file_extension": ".py",
      "version": "3.5.4",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}